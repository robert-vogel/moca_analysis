# Figure Panel 3

The plots in Figure 3 were generated by the scripts and
data in this directory.

To generate new plots run the bash script
`generate_plots.sh`

Note that the Gaussian Mixture Model (GMM) uses scikit-learn's
implementation and assumes diagonal
class conditioned covariance matrices, as this is the
assumption as Umoca.

The score of sample \$k\$ from the GMM were computed by:

```math
s_k^{\text{GMM}} = \sum_{i=1}^M \bigg(\frac{1}{\sigma^2_{i|1}} - \frac{1}{\sigma^2_{i|0}}\bigg) r_{ik}^2
  + 2\bigg(\frac{\mu_{i|0}}{\sigma^2_{i|0}} - \frac{\mu_{i|1}}{\sigma^2_{i|1}}\bigg) r_{ik}
```

where $i$ indexes over methods.  While \$\mu_{i|c}\$ and \$\sigma^2_{i|c}\$ for 
\$c\$ class labels \$c\$ are simply the conditional mean and variant of method \$i\$.
This expression can be derived by Bayes' theorem and the Gaussian density function 
\$f_{i|c}\$.

By Bayes' theorem

```math
\mathbb{P}(C_k=1 | \mathbf{r}_k) = \frac{\mathbb{P}(\mathbf{r}_k|C_k=1)\mathbb{P}(C_k=1)
}{
\sum_{c=0,1}\mathbb{P}(\mathbf{r}_k|C_k=c)\mathbb{P}(C_k=c)
}
```

then, rearranging the terms we see that

```math
\mathbb{P}(C_k=1 | \mathbf{r}_k) = \bigg(1 + 
\frac{\mathbb{P}(\mathbf{r}_k|C_k=0)
}{
\mathbb{P}(\mathbf{r}_k|C_k=1)
}
\frac{
\mathbb{P}(C_k=0)
}{
\mathbb{P}(C_k=1)
}
\bigg)^{-1}
```
where the ratio of prior probabilities is not a function of a sample's 
rank and consequently can be ignored.  By the Gaussian density function
the score of \$k\$ is simply the first and second order terms written above.
