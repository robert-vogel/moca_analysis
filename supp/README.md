# Supplemental Figure 1

The plots in Supplemental Figure 1 were generated by the scripts and
data in this directory.

To generate new plots run the bash script
`generate_plots.sh`

Note that the Gaussian Mixture Model (GMM) uses scikit-learn's
implementation.

The score \$s_k\$ of sample \$k\$ from the GMM was computed by:

```math
s_k^{\text{GMM}} =
\mathbf{r}_k^T
    \bigg(\boldsymbol{\Sigma}^{-1}_1 - \boldsymbol{\Sigma}^{-1}_0\bigg)
    \mathbf{r}_k
+ 2 \bigg(\boldsymbol{\mu}_0^T\boldsymbol{\Sigma}^{-1}_0
    - \boldsymbol{\mu}_1^T\boldsymbol{\Sigma}^{-1}_1\bigg) \mathbf{r}_k
```

with \$\boldsymbol{\mu}_c\$ and \$\boldsymbol{\Sigma}^{-1}_c\$ are
the conditional statistics of the methods given class label \$c\$.
This expression can be derived by Bayes' theorem and the Gaussian density function 
\$f_c\$.

By Bayes' theorem

```math
\mathbb{P}(C_k=1 | \mathbf{r}_k) = \frac{\mathbb{P}(\mathbf{r}_k|C_k=1)\mathbb{P}(C_k=1)
}{
\sum_{c=0,1}\mathbb{P}(\mathbf{r}_k|C_k=c)\mathbb{P}(C_k=c)
}
```

then, rearranging the terms we see that

```math
\mathbb{P}(C_k=1 | \mathbf{r}_k) = \bigg(1 + 
\frac{\mathbb{P}(\mathbf{r}_k|C_k=0)
}{
\mathbb{P}(\mathbf{r}_k|C_k=1)
}
\frac{
\mathbb{P}(C_k=0)
}{
\mathbb{P}(C_k=1)
}
\bigg)^{-1}
```
where the ratio of prior probabilities is not a function of a sample's 
rank and consequently can be ignored.  By the Gaussian density function
the sample score of \$s_k\$ is simply the first and second order terms written above.

